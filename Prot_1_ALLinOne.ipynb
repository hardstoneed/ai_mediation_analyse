{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prototyp v0.1  zur Evaluierung der Analyse von  Mediationstranskipten via LLMs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all Imports, Configs and Regex \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import openai\n",
    "import time \n",
    "import re\n",
    "import inspect\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#load transkripts\n",
    "\n",
    "# Funktion zum Laden des Konversationstexts aus einer Datei\n",
    "def load_conversation_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        conversation_text = file.read()\n",
    "    return conversation_text\n",
    "\n",
    "\n",
    "file_path = 'med.txt'\n",
    "# Konversationstext aus der Datei laden\n",
    "conversation_text_gen = load_conversation_from_file(file_path)\n",
    "\n",
    "file_path = 'realBsp.txt'\n",
    "conversation_text_real = load_conversation_from_file(file_path)\n",
    "\n",
    "# Split the conversation into individual lines\n",
    "lines_gen = [line_gen.strip() for line_gen in conversation_text_gen.split(\"\\n\")]\n",
    "lines_real = [line_real.strip() for line_real in conversation_text_real.split(\"\\n\")]\n",
    "\n",
    "# Remove empty strings\n",
    "lines_gen = list(filter(None, lines_gen))\n",
    "lines_real = list(filter(None, lines_real))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#LLM Configs\n",
    "\n",
    "\n",
    "openai_config_3 = {\n",
    "    'model_name': 'gpt-3.5-turbo',\n",
    "    'key': 'xx'\n",
    "}\n",
    "openai_config_4 = {\n",
    "    'model_name': 'gpt-4',\n",
    "    'key': 'xx'\n",
    "}\n",
    "openai_config_13B = {\n",
    "    'model_name': 'llama2 13B',\n",
    "    'base': 'http://k3:4321/v1', #works on workstation k3\n",
    "    'key': 'cc' \n",
    "}\n",
    "openai_config_70B = {\n",
    "    'model_name': 'llama2 70B',\n",
    "    'base': 'http://k3:1234/v1', #works on workstation k3\n",
    "     'key': 'cc'\n",
    "}\n",
    "openai_config_pol  = {\n",
    "    'model_name': 'polarity',\n",
    "    'pol':1 \n",
    " \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistische Analyse der Redeanteile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a dictionary to map speakers to colors\n",
    "speaker_colors = {\n",
    "        \"Sprecher I\": \"orange\",\n",
    "        \"Sprecher B1\": \"olive\",\n",
    "        \"Sprecher B2\": \"cornflowerblue\",\n",
    "        \"Sprecher M\": \"orange\",\n",
    "        \"Sprecher B\": \"olive\",\n",
    "        \"Sprecher A\": \"cornflowerblue\",\n",
    "        # Add more speaker-color mappings here\n",
    "    }\n",
    "\n",
    "def extract_speaker_stats(conversation_text, title):\n",
    "\n",
    "    # Extract speakers and count the total number of words for each speaker\n",
    "    sprecher_beitraege = re.split(r'(?=[A-Z]+\\d*:)', conversation_text)\n",
    "    gesamt_woerter_pro_sprecher = {}\n",
    "\n",
    "    for beitrag in sprecher_beitraege:\n",
    "        sprecher_match = re.match(r'([A-Z]+\\d*):', beitrag)\n",
    "        if sprecher_match:\n",
    "            sprecher = f\"Sprecher {sprecher_match.group(1)}\"\n",
    "            gesamt_woerter_pro_sprecher[sprecher] = len(re.findall(r'\\w+', beitrag))\n",
    "\n",
    "    # Calculate the percentage of speech time\n",
    "    gesamt_woerter = sum(gesamt_woerter_pro_sprecher.values())\n",
    "    prozentuale_redeanteile = {sprecher: (woerter / gesamt_woerter) * 100 for sprecher, woerter in gesamt_woerter_pro_sprecher.items()}\n",
    "\n",
    "    # Extract the speaker names from the dictionary keys\n",
    "    speakers = prozentuale_redeanteile.keys()\n",
    "\n",
    "    # Get the colors for each speaker based on the speaker-color dictionary\n",
    "    colors = [speaker_colors.get(speaker, \"gray\") for speaker in speakers]\n",
    "\n",
    "    # Plot the pie chart with the assigned colors\n",
    "    plt.pie(prozentuale_redeanteile.values(), labels=speakers, colors=colors, autopct='%1.1f%%')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Generate pie chart for 'generierte Mediation'\n",
    "extract_speaker_stats(conversation_text_gen, 'Prozentualer Redeanteil nach Sprechern für generierte Mediation')\n",
    "\n",
    "# Generate pie chart for 'realen Mediation'\n",
    "extract_speaker_stats(conversation_text_real, 'Prozentualer Redeanteil nach Sprechern für realen Mediation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequenzielle Sentiment Analysen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "\n",
    "def extract_float(text):\n",
    "    numbers = re.findall(r'-?\\d+\\.\\d+', text)\n",
    "    return float(numbers[0]) if numbers else 0\n",
    "\n",
    "#Function for send Prompt to LLM \n",
    "def ask_ki(prompt,model_name):\n",
    "    # Send the user prompt to ChatGPT using the correct endpoint for chat completions\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=100)#,\n",
    "        #temperature = 0)#,\n",
    "        #top_k=3)    # Adjust as needed\n",
    "    return response\n",
    "\n",
    "\n",
    "def analyze_sentiments(openai_config, lines,file_name, table_Sent):\n",
    "    # Startzeit für die Messung der benötigten Zeit\n",
    "    start_time = time.time()\n",
    " \n",
    "    # Recognize speakers based on lines starting with a single uppercase letter followed by a colon\n",
    "    \n",
    "\n",
    "    # Initialize dictionaries to store sentiment and speaker parts\n",
    "    sentiment_per_speaker = defaultdict(list)\n",
    "    speaker_lengths = {}\n",
    "    sentiments_over_time = defaultdict(list)\n",
    "    if 'model_name' in openai_config:\n",
    "        model_name = openai_config['model_name']\n",
    "    if 'key' in openai_config:\n",
    "        openai.api_key = openai_config['key']\n",
    "    \n",
    "    if 'base' in openai_config:\n",
    "        openai.api_base = openai_config['base']\n",
    "\n",
    "    \n",
    "    current_speaker = None\n",
    "    # Analyze sentiment for each line using the OpenAI API\n",
    "    for idx, line in enumerate(lines, start=1):\n",
    "    # Check if the line starts with a single uppercase letter followed by a colon\n",
    "        \n",
    "                \n",
    "       # if len(line) > 2 and line[1] == ':' and line[0].isalpha() and line[0].isupper():\n",
    "          # current_speaker = line.split(':')[0].strip()\n",
    "\n",
    "             \n",
    "        if len(line) > 3:\n",
    "            match = re.match(r'^([A-Z\\d]+):', line)\n",
    "            if match:\n",
    "                current_speaker = match.group(1)\n",
    "\n",
    "        #line = re.sub(r'\\(\\d+\\.\\d+\\)$', '', line)\n",
    "\n",
    "        if 'pol' in openai_config:\n",
    "               sentiment_value= TextBlob(line).sentiment.polarity \n",
    "        else:\n",
    "            # User prompt for ChatGPT, including the sentiment analysis request\n",
    "            prompt = f\"{current_speaker}: {line}\\n Rate the sentiment on a scale from -1 to 1, Give only the Float Number as result: \"\n",
    "            response = ask_ki(prompt,model_name)\n",
    "    \n",
    "       \n",
    "             # Extract sentiment information from the response\n",
    "            sentiment_label = response['choices'][0]['message']['content'].strip()\n",
    "            sentiment_value= extract_float(sentiment_label)\n",
    "    \n",
    "        #Datentabelle\n",
    "        #print(f\"Sprecher: {current_speaker} Text: {line} Sentiment: {sentiment_value} \")  \n",
    "\n",
    "        \n",
    "        # Überprüfen, ob die Zeile existiert (zum Beispiel anhand des Namens)\n",
    "        if not any(table_Sent['idx'] == idx):\n",
    "         # Füge eine neue Zeile für  hinzu, falls sie nicht existiert\n",
    "                table_Sent = table_Sent.append({'idx': idx, 'Sprecher': current_speaker, 'Text': line}, ignore_index=True)\n",
    "\n",
    "        # Wert für eine bestimmte Zelle setzen, auch wenn die Zeile gerade erst hinzugefügt wurde\n",
    "        table_Sent.at[table_Sent[table_Sent['idx'] == idx].index[0], openai_config['model_name']] = sentiment_value\n",
    "\n",
    "        #print(table_Sent) \n",
    "\n",
    "        # Store sentiment information\n",
    "        sentiment_per_speaker[current_speaker].append(sentiment_value)\n",
    "        \n",
    "        # Store speaker part lengths\n",
    "        speaker_lengths[current_speaker] = len(line)\n",
    "    \n",
    "        # Store sentiments over time\n",
    "        sentiments_over_time[current_speaker].append({\"sentiment\": sentiment_value, \"time\": idx})\n",
    "\n",
    "\n",
    "\n",
    "        # Create a timeline chart for each speaker\n",
    "    for speaker, time_sentiments in sentiments_over_time.items():\n",
    "        times = [entry[\"time\"] for entry in time_sentiments]\n",
    "        sentiments = [entry[\"sentiment\"] for entry in time_sentiments]\n",
    "        plt.plot(times, sentiments, label=speaker)\n",
    "\n",
    "    # Berechne die verbrauchten Tokens\n",
    "    if 'pol' in openai_config:\n",
    "        consumed_tokens = 0\n",
    "    else:    \n",
    "        consumed_tokens = response['usage']['total_tokens']\n",
    "\n",
    "    #  Endzeit für die Messung der benötigten Zeit\n",
    "    end_time = time.time()\n",
    "\n",
    "\n",
    "    # Berechne die benötigte Zeit\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    \n",
    "    print(f'Consumed Tokens: {consumed_tokens} - Elapsed Time: {elapsed_time:.2f} seconds')\n",
    "    plt.xlabel('Line Number')\n",
    "    plt.ylabel('Sentiment')\n",
    "    plt.title(f'Sentiments Over Time - Model: {model_name} - File: {file_name} ')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return table_Sent\n",
    "\n",
    "#perform the analyse\n",
    "file_name = \"med.txt\"\n",
    "\n",
    "#Datentabelle für Gen\n",
    "table_Sent_gen = pd.DataFrame(columns=['idx','Sprecher', 'Text', openai_config_3['model_name'],openai_config_4['model_name'], openai_config_13B['model_name'], openai_config_70B['model_name'],openai_config_pol['model_name']])\n",
    "\n",
    "\n",
    "\n",
    "print(\"gen GPT3.5\")\n",
    "table_Sent_gen = analyze_sentiments(openai_config_3, lines_gen, file_name, table_Sent_gen)\n",
    "print(\"gen GPT4\")\n",
    "table_Sent_gen = analyze_sentiments(openai_config_4, lines_gen, file_name, table_Sent_gen)\n",
    "print(\"gen 13B\")\n",
    "table_Sent_gen = analyze_sentiments(openai_config_13B, lines_gen, file_name, table_Sent_gen)\n",
    "\n",
    "print(\"gen 70B\")\n",
    "table_Sent_gen =  analyze_sentiments(openai_config_70B, lines_gen, file_name, table_Sent_gen)\n",
    "print(\"gen pol\")\n",
    "table_Sent_gen = analyze_sentiments(openai_config_pol, lines_gen, file_name, table_Sent_gen)\n",
    "\n",
    "print(table_Sent_gen) \n",
    "table_Sent_gen.to_excel(\"sentimen_gen.xlsx\", index=False)\n",
    "\n",
    "\n",
    "#realBsp --> only local LLM 13B and 70B \n",
    "\n",
    "table_Sent_real = pd.DataFrame(columns=['idx','Sprecher', 'Text', openai_config_13B['model_name'], openai_config_70B['model_name'],openai_config_pol['model_name']])\n",
    "\n",
    "file_name = \"realBsp.txt\"\n",
    "print(\"real 13B\")\n",
    "table_Sent_real = analyze_sentiments(openai_config_13B, lines_real, file_name ,table_Sent_real)\n",
    "print(\"real 70B\")\n",
    "table_Sent_real = analyze_sentiments(openai_config_70B, lines_real, file_name,table_Sent_real)\n",
    "print(\"real pol\")\n",
    "table_Sent_real = analyze_sentiments(openai_config_pol, lines_real, file_name,table_Sent_real)\n",
    "print(table_Sent_real)\n",
    "table_Sent_real.to_excel(\"sentimen_real.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kontextbasierte Fragen an das LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "\n",
    "# Spezifische Fragen\n",
    "questions = [\n",
    "    \"Which mediation technique is used in the following conversation?\",\n",
    "    \"What was the spirit like during the mediation? \",\n",
    "    \"What were the main reasons for the conflict in the mediationtranscription? \",\n",
    "    \"How much of the conversation does each participant have?\",\n",
    "    \"Is the mediation technique appropriate, or is there room for improvement?\"\n",
    "]\n",
    "\n",
    "temperature=0.0\n",
    "\n",
    "\n",
    "# Send the user prompt to ChatGPT using the chat/completions endpoint\n",
    "def ask_question_to_gpt(prompt,model_name):\n",
    "     \n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=model_name,      \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant and give short correct answers\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    #max_tokens=-1,\n",
    "    temperature = temperature,\n",
    "    timeout=1000,\n",
    "    top_k=2\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "\n",
    "\n",
    "def ask_questions(openai_config, conversation_text, table_Quest):\n",
    "    if 'model_name' in openai_config:\n",
    "        openai.model_name = openai_config['model_name']\n",
    "        \n",
    "    if 'key' in openai_config:\n",
    "        openai.api_key = openai_config['key']\n",
    "    if 'base' in openai_config:\n",
    "        openai.api_base = openai_config['base']\n",
    "  \n",
    "    for question in questions:\n",
    "        answer = ask_question_to_gpt(f\"Frage: {question} zur Mediation: [{conversation_text}]\",openai_config['model_name'])\n",
    "        if not any(table_Quest['Frage'] == question):\n",
    "         # Füge eine neue Zeile für  hinzu, falls sie nicht existiert\n",
    "                table_Quest = table_Quest.append({'Frage': question }, ignore_index=True)\n",
    "\n",
    "        # Wert für eine bestimmte Zelle setzen, auch wenn die Zeile gerade erst hinzugefügt wurde\n",
    "        table_Quest.at[table_Quest[table_Quest['Frage'] == question].index[0], openai_config['model_name']] = answer\n",
    "        print(f\"Frage: {question}\\nAntwort: {answer}\\n\")\n",
    "\n",
    "\n",
    "    # Drucken des Models und der Quelle\n",
    "    print(f\"Model: {openai_config['model_name']} - \\n\")\n",
    "    return table_Quest\n",
    "\n",
    "\n",
    "#perform the analyse\n",
    "\n",
    "file_name = \"med.txt\"\n",
    "table_Quest_gen = pd.DataFrame(columns=['Frage', openai_config_3['model_name'],openai_config_4['model_name'], openai_config_13B['model_name'], openai_config_70B['model_name']])\n",
    "\n",
    "print(\"gen GPT3.5\")\n",
    "#table_Quest_gen = ask_questions(openai_config_3, conversation_text_gen, table_Quest_gen)\n",
    "print(\"gen GPT4\")\n",
    "#table_Quest_gen = ask_questions(openai_config_4, conversation_text_gen, table_Quest_gen)\n",
    "print(\"gen 13B\")\n",
    "#table_Quest_gen = ask_questions(openai_config_13B, conversation_text_gen, table_Quest_gen)\n",
    "print(\"gen 70B\")\n",
    "table_Quest_gen = ask_questions(openai_config_70B, conversation_text_gen, table_Quest_gen)\n",
    "print(table_Quest_gen)\n",
    "\n",
    "#table_Quest_gen.to_excel(\"questian_gen.xlsx\", index=False)\n",
    "\n",
    "file_name = \"realBsp.txt\"\n",
    "#realBsp --> only local LLM 13B and 70B \n",
    "table_Quest_real = pd.DataFrame(columns=['Frage', openai_config_13B['model_name'], openai_config_70B['model_name']])\n",
    "\n",
    "print(\"real 13B\")\n",
    "#table_Quest_real = ask_questions(openai_config_13B, conversation_text_real, table_Quest_real)\n",
    "print(\"real 70B\")\n",
    "table_Quest_real = ask_questions(openai_config_70B, conversation_text_real, table_Quest_real)\n",
    "\n",
    "table_Quest_real.to_excel(\"questian_real.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgabe der Datentabellen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nDatentabelle Sentiment für generierte Mediation\\n')\n",
    "print(table_Sent_gen)\n",
    "\n",
    "print('\\nDatentabelle Sentiment für reale Mediation\\n')\n",
    "print(table_Sent_real)\n",
    "\n",
    "print('\\nDatentabelle Fragen für generierte Mediation\\n')\n",
    "print(table_Quest_gen)\n",
    "\n",
    "print('\\nDatentabelle Fragen für reale Mediation\\n')\n",
    "print(table_Quest_real)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
