{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mediationsprototype 2.2 \n",
    " Version 2 Teil 2 Anonymisierung Presidio \n",
    "\n",
    "daten /ppm werden nicht im Bitbucket gehalten (Privacy)\n",
    "anonymizierte Daten  in /anon  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module install \n",
    "#!pip install presidio_analyzer presidio_anonymizer\n",
    "#!python -m spacy download de_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import re\n",
    "from striprtf.striprtf import rtf_to_text\n",
    "\n",
    "from presidio_analyzer import AnalyzerEngine, RecognizerRegistry, PatternRecognizer\n",
    "from presidio_analyzer.nlp_engine import NlpEngineProvider, TransformersNlpEngine\n",
    "\n",
    "\n",
    "#from transformers import pipeline\n",
    "from presidio_anonymizer import AnonymizerEngine, DeanonymizeEngine, OperatorConfig\n",
    "from presidio_anonymizer.operators import Operator, OperatorType\n",
    "\n",
    "from typing import Dict\n",
    "from pprint import pprint\n",
    "\n",
    "import transformers\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorbereiten der rtf\n",
    "Die Quellscripte werden von technischen Steuerzeichen befreite und als UTF8 im txt Format vorbereitet.\n",
    "Es wird für jede Sitzung eine Datei erstellt. _S1.txt .. _S6.txt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiere die Verzeichnisse und Dateinamen\n",
    "Fall= \"F8\"\n",
    "Sitzungsanzahl = 5\n",
    "input_dir = fr'ppm\\{Fall}'  # Achte darauf, dass dieser Pfad existiert und korrekt ist\n",
    "output_dir = fr'ppm\\{Fall}'\n",
    "output_file_path = os.path.join(output_dir, '_S')\n",
    "\n",
    "# Überprüfen, ob das Zielverzeichnis existiert, und es ggf. erstellen\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Liste der RTF-Dateinamen ausgeben\n",
    "print(f\"Suche nach RTF-Dateien in: {input_dir}\")\n",
    "\n",
    "# RTF-Dateien finden und deren Inhalt bereinigen\n",
    "rtf_files = [f'S{i}.rtf' for i in range(1, Sitzungsanzahl+1)]  # Liste mit Dateinamen von S1.rtf bis S6.rtf\n",
    "extracted_texts = []  # Sammlung für bereinigte Texte\n",
    "\n",
    "i = 1\n",
    "for rtf_file in rtf_files:\n",
    "    file_path = os.path.join(input_dir, rtf_file)\n",
    "    if os.path.isfile(file_path):\n",
    "        print(f\"Verarbeite Datei: {file_path}\")\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            rtf_content = file.read()\n",
    "            text = rtf_to_text(rtf_content)  # RTF-Steuerdaten entfernen\n",
    "            cleaned_text = '\\n'.join(filter(lambda x: x.strip(), text.splitlines()))\n",
    "            #extracted_texts.append(cleaned_text)\n",
    "            with open(f\"{output_file_path}{i}.txt\" , 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(''.join(cleaned_text))\n",
    "                print(f\"Die Inhalte wurden erfolgreich in '{output_file_path}{i}' gespeichert.\")\n",
    "    else:\n",
    "        print(f\"Datei nicht gefunden: {file_path}\")\n",
    "    i += 1\n",
    "\n",
    "\n",
    "# Bereinigte RTF-Inhalte in eine Ausgabedatei schreiben\n",
    "if extracted_texts:\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        output_file.write('\\n'.join(extracted_texts))\n",
    "    print(f\"Die Inhalte wurden erfolgreich in '{output_file_path}'.txt gespeichert.\")\n",
    "else:\n",
    "    print(\"Keine RTF-Inhalte zum Speichern gefunden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PII Analyse und  Anonymisierung\n",
    "Die Analyse erfolt mit den Frameworks Spacy, Transformer und MS Presidio mit verschiedenen Identifikationsmethoden. Zudem werden Manuell Identifizierte Eigennamen und Orte als Liste hinzugefügt, da Testreihen bestimmte Eigenamen unzuverlässig erkannt haben. Hier ist Finetuning und Training der Modelle möglich, allerdings nicht Fokus dieser Untersuchung. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# laden der Transkripte\n",
    "text_list = []\n",
    "for i in range(1, Sitzungsanzahl+1):  # Annahme: _S1.txt bis _S6.txt\n",
    "    filename = f'_S{i}.txt'\n",
    "    input_file_path = os.path.join(input_dir, filename)\n",
    "    print(f\"Transkript : {input_file_path} geladen\")\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        text_list.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konfiguration von Presidio \n",
    "hier werden zudem die bekannten Entitäten und die Ausnahmen definiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the Models\n",
    "transformers_model = \"domischwimmbeck/bert-base-german-cased-fine-tuned-ner\"\n",
    "#snapshot_download(repo_id=transformers_model)\n",
    "# Instantiate to make sure it's downloaded during installation and not runtime\n",
    "AutoTokenizer.from_pretrained(transformers_model)\n",
    "AutoModelForTokenClassification.from_pretrained(transformers_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Presidio-Konfiguration\n",
    "\n",
    "# Bekannte Namen und Ort der Mediationen hier \n",
    "known_names_list = []\n",
    "known_locations_list = []\n",
    "\n",
    "#Whitelist --> alles was nicht als PII erkannt werden soll \n",
    "allow_list = [\"Afrika\",\"Tschüss\",\"I\",\"TASCHENGELD\",\"TAG\",\"ECHT\",\"IHM\",\"IHNEN\",\"GEMEINSAM\",\"GEGENWART\",\"DADRIN\",\"AUCH\",\"BANK\",\"Google\",\"England\",\"Europapark\",\"Himalaya\",\"Landschulheim\",\"Schweiz\",\"Tierpark\",\"Instagram\",\"GRUND\",\"GANZ\",\"ICH\",\"FÜNF\",\"MIR\",\"Wischiwaschi\",\"Weil\",\"Uroma\",\"Trulla\",\"TAKTISCH\",\"Ja\",\"DU\",\"Aha\",\"SACHE\",\"KINDER\",\"Instagram\",\"DOCH\",\"Amazon\",\"Corona\",\"Flipchart\",\"Flipcharts\",\"Mensa Kids\",\"YouTube\",\"Stadt\",\"B1\",\"B2\",\"I2\",\"I1\",\"Land\",\"Scholli\",\"Ähm\",\"IMMER\",\"SELBER\",\"/\",\"WÜRDEN\",\"BOAH\",\"VW\",\"BIS\",\"GUT\",\"Schnieft\",\"Boah\"]           \n",
    "PRESIDIO_ENTITIES = [\"PERSON\", \"LOCATION\", \"ORGANIZATION\", \"EMAIL_ADDRESS\", \"IBAN_CODE\", \"PHONE_NUMBER\"]\n",
    "\n",
    "#spacy or transformers in configfile\n",
    "NLP_CONFIG_FILE = \"nlpconf.yml\" \n",
    "\n",
    "# Create a PatternRecognizer for the deny list  \n",
    "deny_list_recognizer_loc = PatternRecognizer(supported_entity=\"LOCATION\", supported_language= \"de\", deny_list=known_locations_list)\n",
    "deny_list_recognizer_per = PatternRecognizer(supported_entity=\"PERSON\", supported_language= \"de\", deny_list=known_names_list)\n",
    "\n",
    "registry = RecognizerRegistry()\n",
    "registry.load_predefined_recognizers(languages=[\"de\"])\n",
    "registry.add_recognizer(deny_list_recognizer_loc)\n",
    "registry.add_recognizer(deny_list_recognizer_per)\n",
    "\n",
    "provider = NlpEngineProvider(conf_file=NLP_CONFIG_FILE)\n",
    "nlp_engine = provider.create_engine()\n",
    "analyzer = AnalyzerEngine(registry=registry, supported_languages=[\"de\"], nlp_engine=nlp_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstanceCounterAnonymizer(Operator):\n",
    "    \"\"\"\n",
    "    Anonymizer which replaces the entity value\n",
    "    with an instance counter per entity.\n",
    "    \"\"\"\n",
    "\n",
    "    REPLACING_FORMAT = \"<{entity_type}_{index}>\"\n",
    "\n",
    "    def operate(self, text: str, params: Dict = None) -> str:\n",
    "        \"\"\"Anonymize the input text.\"\"\"\n",
    "\n",
    "        entity_type: str = params[\"entity_type\"]\n",
    "\n",
    "        # entity_mapping is a dict of dicts containing mappings per entity type\n",
    "        entity_mapping: Dict[Dict:str] = params[\"entity_mapping\"]\n",
    "\n",
    "        entity_mapping_for_type = entity_mapping.get(entity_type)\n",
    "        if not entity_mapping_for_type:\n",
    "            new_text = self.REPLACING_FORMAT.format(\n",
    "                entity_type=entity_type, index=0\n",
    "            )\n",
    "            entity_mapping[entity_type] = {}\n",
    "\n",
    "        else:\n",
    "            if text in entity_mapping_for_type:\n",
    "                return entity_mapping_for_type[text]\n",
    "\n",
    "            previous_index = self._get_last_index(entity_mapping_for_type)\n",
    "            new_text = self.REPLACING_FORMAT.format(\n",
    "                entity_type=entity_type, index=previous_index + 1\n",
    "            )\n",
    "\n",
    "        entity_mapping[entity_type][text] = new_text\n",
    "        return new_text\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_last_index(entity_mapping_for_type: Dict) -> int:\n",
    "        \"\"\"Get the last index for a given entity type.\"\"\"\n",
    "\n",
    "        def get_index(value: str) -> int:\n",
    "            return int(value.split(\"_\")[-1][:-1])\n",
    "\n",
    "        indices = [get_index(v) for v in entity_mapping_for_type.values()]\n",
    "        return max(indices)\n",
    "\n",
    "    def validate(self, params: Dict = None) -> None:\n",
    "        \"\"\"Validate operator parameters.\"\"\"\n",
    "\n",
    "        if \"entity_mapping\" not in params:\n",
    "            raise ValueError(\"An input Dict called `entity_mapping` is required.\")\n",
    "        if \"entity_type\" not in params:\n",
    "            raise ValueError(\"An entity_type param is required.\")\n",
    "\n",
    "    def operator_name(self) -> str:\n",
    "        return \"entity_counter\"\n",
    "\n",
    "    def operator_type(self) -> OperatorType:\n",
    "        return OperatorType.Anonymize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse und Ananomize Funktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymize_text_with_presidio2(text, analyzer_result):\n",
    "    anonymizer_engine = AnonymizerEngine()\n",
    "    anonymizer_engine.add_anonymizer(InstanceCounterAnonymizer)\n",
    "    global entity_mapping\n",
    "    anonymized_result = anonymizer_engine.anonymize(text=text, analyzer_results= analyzer_result,\n",
    "                                          operators={ \"DEFAULT\": OperatorConfig(\n",
    "                                                \"entity_counter\", {\"entity_mapping\": entity_mapping})})\n",
    "    anonymized_text = anonymized_result.text\n",
    "    return anonymized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ausführen der Analysen und Anonymisierungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overwrite textlist\n",
    "#text_list = [\"Bernd wohnt in Berlin. Max Mustermann arbeitet bei Bosch und wohnt in (Karlsruhe?). in der Auenstrasse 3. Max hat aber auch Verwandte in Berlin.\"]\n",
    "# Create a mapping between entity types and counters\n",
    "entity_mapping = dict()\n",
    "anonymized_text_presidio_list = []\n",
    "global CountFind\n",
    "CountFind = 0\n",
    "for text in text_list:\n",
    "    #presidio\n",
    "    analyser_pres = analyzer.analyze(text=text, language=\"de\", entities=PRESIDIO_ENTITIES, score_threshold=0.5,return_decision_process = True, allow_list = allow_list)\n",
    "    CountFind += len(analyser_pres)\n",
    "    #for results in analyser_pres:\n",
    "        #print(f\"Finding {results}  mit {results.analysis_explanation}\")\n",
    "    anonymized_text_presidio_list.append(anonymize_text_with_presidio2(text, analyser_pres))\n",
    "    #print(anonymized_text_presidio_list[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auswertung und Speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,anon_text in enumerate(anonymized_text_presidio_list):\n",
    "    print(f\"\\nAnonymisierter Text (Presidio){index+1}:\\n{anon_text[:200]}\")\n",
    "\n",
    "print(f\"mapping {CountFind}:\\n\")\n",
    "\n",
    "# Festelegen des Verzeihnisses für Produktivumgebung\n",
    "output_dir = fr'anon\\{Fall}'\n",
    "pprint(entity_mapping, indent=2)\n",
    "#save\n",
    "for index,anon_text in enumerate(anonymized_text_presidio_list):\n",
    "    anon_output_file_path = os.path.join(output_dir, f'{Fall}_{index+1}.txt')        \n",
    "    with open(anon_output_file_path, 'w', encoding='utf-8') as file:    \n",
    "        file.write(anon_text)\n",
    "print(\"Anonymisierte Transkriptionen gespeichert\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
